{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    },
    "colab": {
      "name": "Training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDog8VU_USgS"
      },
      "source": [
        "# Training data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULcyinkqUT_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87e754d2-d082-495b-9828-8d8923836fdc"
      },
      "source": [
        "# Module for intents json file\n",
        "import json\n",
        "\n",
        "# Modules for Natural Language Processing\n",
        "import nltk\n",
        "import pickle\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Modules for Vectorization and Encoding\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Modules for Neural Networks\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Module for random responses\n",
        "import random\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "intents = json.loads(open('intents.json').read())\n",
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore_letters = ['?', '!', '.', ',']\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "    word_list = nltk.word_tokenize(pattern)\n",
        "    words.extend(word_list)\n",
        "    documents.append((word_list ,intent['tag']))\n",
        "    if intent['tag'] not in classes:\n",
        "      classes.append(intent['tag'])\n",
        "\n",
        "words = [lemmatizer.lemmatize(word) for word in words if word  not in ignore_letters]\n",
        "words = sorted(set(words))\n",
        "\n",
        "classes = sorted(set(classes))\n",
        "\n",
        "pickle.dump(words, open('words.pkl', 'wb'))\n",
        "pickle.dump(classes, open('classes.pkl', 'wb'))\n",
        "\n",
        "training = []\n",
        "output_empty = [0] * len(classes)\n",
        "\n",
        "for document in documents:\n",
        "  bag = []\n",
        "  word_patterns = document[0]\n",
        "  word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
        "  for word in words:\n",
        "    bag.append(1) if word in word_patterns else bag.append(0)\n",
        "\n",
        "  output_row = list(output_empty)\n",
        "  output_row[classes.index(document[1])] = 1\n",
        "  training.append([bag, output_row])\n",
        "\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "train_x = list(training[:, 0])\n",
        "train_y = list(training[:, 1])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation= 'relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(len(train_y[0]), activation= 'softmax'))\n",
        "\n",
        "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
        "\n",
        "hist = model.fit(np.array(train_x), np.array(train_y),epochs=20, batch_size=5, verbose=1)\n",
        "model.save('chatbot_model.h5', hist)\n",
        "print('Done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/optimizer_v2/optimizer_v2.py:375: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  \"The `lr` argument is deprecated, use `learning_rate` instead.\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "2/2 [==============================] - 1s 9ms/step - loss: 0.7286 - accuracy: 0.5000\n",
            "Epoch 2/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6652 - accuracy: 0.5000\n",
            "Epoch 3/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6559 - accuracy: 0.6000\n",
            "Epoch 4/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6763 - accuracy: 0.5000\n",
            "Epoch 5/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6305 - accuracy: 0.6000\n",
            "Epoch 6/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.6255 - accuracy: 0.6000\n",
            "Epoch 7/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5905 - accuracy: 0.6000\n",
            "Epoch 8/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6539 - accuracy: 0.7000\n",
            "Epoch 9/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5670 - accuracy: 0.7000\n",
            "Epoch 10/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5631 - accuracy: 0.7000\n",
            "Epoch 11/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5865 - accuracy: 0.6000\n",
            "Epoch 12/20\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5550 - accuracy: 0.7000\n",
            "Epoch 13/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5565 - accuracy: 0.6000\n",
            "Epoch 14/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5720 - accuracy: 0.7000\n",
            "Epoch 15/20\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5610 - accuracy: 0.6000\n",
            "Epoch 16/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5457 - accuracy: 0.6000\n",
            "Epoch 17/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4977 - accuracy: 0.6000\n",
            "Epoch 18/20\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5093 - accuracy: 0.6000\n",
            "Epoch 19/20\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4545 - accuracy: 0.6000\n",
            "Epoch 20/20\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5145 - accuracy: 0.5000\n",
            "Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1TUqaYoUSgS"
      },
      "source": [
        "*Jake VanderPlas*\n",
        "\n",
        "![Book Cover](https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/figures/PDSH-cover.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nZ5hQBsUSgS"
      },
      "source": [
        "This is the Jupyter notebook version of the [Python Data Science Handbook](http://shop.oreilly.com/product/0636920034919.do) by Jake VanderPlas; the content is available [on GitHub](https://github.com/jakevdp/PythonDataScienceHandbook).*\n",
        "The text is released under the [CC-BY-NC-ND license](https://creativecommons.org/licenses/by-nc-nd/3.0/us/legalcode), and code is released under the [MIT license](https://opensource.org/licenses/MIT). If you find this content useful, please consider supporting the work by [buying the book](http://shop.oreilly.com/product/0636920034919.do)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co2yXmUrUSgT"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "### [Preface](00.00-Preface.ipynb)\n",
        "\n",
        "### [1. IPython: Beyond Normal Python](01.00-IPython-Beyond-Normal-Python.ipynb)\n",
        "- [Help and Documentation in IPython](01.01-Help-And-Documentation.ipynb)\n",
        "- [Keyboard Shortcuts in the IPython Shell](01.02-Shell-Keyboard-Shortcuts.ipynb)\n",
        "- [IPython Magic Commands](01.03-Magic-Commands.ipynb)\n",
        "- [Input and Output History](01.04-Input-Output-History.ipynb)\n",
        "- [IPython and Shell Commands](01.05-IPython-And-Shell-Commands.ipynb)\n",
        "- [Errors and Debugging](01.06-Errors-and-Debugging.ipynb)\n",
        "- [Profiling and Timing Code](01.07-Timing-and-Profiling.ipynb)\n",
        "- [More IPython Resources](01.08-More-IPython-Resources.ipynb)\n",
        "\n",
        "### [2. Introduction to NumPy](02.00-Introduction-to-NumPy.ipynb)\n",
        "- [Understanding Data Types in Python](02.01-Understanding-Data-Types.ipynb)\n",
        "- [The Basics of NumPy Arrays](02.02-The-Basics-Of-NumPy-Arrays.ipynb)\n",
        "- [Computation on NumPy Arrays: Universal Functions](02.03-Computation-on-arrays-ufuncs.ipynb)\n",
        "- [Aggregations: Min, Max, and Everything In Between](02.04-Computation-on-arrays-aggregates.ipynb)\n",
        "- [Computation on Arrays: Broadcasting](02.05-Computation-on-arrays-broadcasting.ipynb)\n",
        "- [Comparisons, Masks, and Boolean Logic](02.06-Boolean-Arrays-and-Masks.ipynb)\n",
        "- [Fancy Indexing](02.07-Fancy-Indexing.ipynb)\n",
        "- [Sorting Arrays](02.08-Sorting.ipynb)\n",
        "- [Structured Data: NumPy's Structured Arrays](02.09-Structured-Data-NumPy.ipynb)\n",
        "\n",
        "### [3. Data Manipulation with Pandas](03.00-Introduction-to-Pandas.ipynb)\n",
        "- [Introducing Pandas Objects](03.01-Introducing-Pandas-Objects.ipynb)\n",
        "- [Data Indexing and Selection](03.02-Data-Indexing-and-Selection.ipynb)\n",
        "- [Operating on Data in Pandas](03.03-Operations-in-Pandas.ipynb)\n",
        "- [Handling Missing Data](03.04-Missing-Values.ipynb)\n",
        "- [Hierarchical Indexing](03.05-Hierarchical-Indexing.ipynb)\n",
        "- [Combining Datasets: Concat and Append](03.06-Concat-And-Append.ipynb)\n",
        "- [Combining Datasets: Merge and Join](03.07-Merge-and-Join.ipynb)\n",
        "- [Aggregation and Grouping](03.08-Aggregation-and-Grouping.ipynb)\n",
        "- [Pivot Tables](03.09-Pivot-Tables.ipynb)\n",
        "- [Vectorized String Operations](03.10-Working-With-Strings.ipynb)\n",
        "- [Working with Time Series](03.11-Working-with-Time-Series.ipynb)\n",
        "- [High-Performance Pandas: eval() and query()](03.12-Performance-Eval-and-Query.ipynb)\n",
        "- [Further Resources](03.13-Further-Resources.ipynb)\n",
        "\n",
        "### [4. Visualization with Matplotlib](04.00-Introduction-To-Matplotlib.ipynb)\n",
        "- [Simple Line Plots](04.01-Simple-Line-Plots.ipynb)\n",
        "- [Simple Scatter Plots](04.02-Simple-Scatter-Plots.ipynb)\n",
        "- [Visualizing Errors](04.03-Errorbars.ipynb)\n",
        "- [Density and Contour Plots](04.04-Density-and-Contour-Plots.ipynb)\n",
        "- [Histograms, Binnings, and Density](04.05-Histograms-and-Binnings.ipynb)\n",
        "- [Customizing Plot Legends](04.06-Customizing-Legends.ipynb)\n",
        "- [Customizing Colorbars](04.07-Customizing-Colorbars.ipynb)\n",
        "- [Multiple Subplots](04.08-Multiple-Subplots.ipynb)\n",
        "- [Text and Annotation](04.09-Text-and-Annotation.ipynb)\n",
        "- [Customizing Ticks](04.10-Customizing-Ticks.ipynb)\n",
        "- [Customizing Matplotlib: Configurations and Stylesheets](04.11-Settings-and-Stylesheets.ipynb)\n",
        "- [Three-Dimensional Plotting in Matplotlib](04.12-Three-Dimensional-Plotting.ipynb)\n",
        "- [Geographic Data with Basemap](04.13-Geographic-Data-With-Basemap.ipynb)\n",
        "- [Visualization with Seaborn](04.14-Visualization-With-Seaborn.ipynb)\n",
        "- [Further Resources](04.15-Further-Resources.ipynb)\n",
        "\n",
        "### [5. Machine Learning](05.00-Machine-Learning.ipynb)\n",
        "- [What Is Machine Learning?](05.01-What-Is-Machine-Learning.ipynb)\n",
        "- [Introducing Scikit-Learn](05.02-Introducing-Scikit-Learn.ipynb)\n",
        "- [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb)\n",
        "- [Feature Engineering](05.04-Feature-Engineering.ipynb)\n",
        "- [In Depth: Naive Bayes Classification](05.05-Naive-Bayes.ipynb)\n",
        "- [In Depth: Linear Regression](05.06-Linear-Regression.ipynb)\n",
        "- [In-Depth: Support Vector Machines](05.07-Support-Vector-Machines.ipynb)\n",
        "- [In-Depth: Decision Trees and Random Forests](05.08-Random-Forests.ipynb)\n",
        "- [In Depth: Principal Component Analysis](05.09-Principal-Component-Analysis.ipynb)\n",
        "- [In-Depth: Manifold Learning](05.10-Manifold-Learning.ipynb)\n",
        "- [In Depth: k-Means Clustering](05.11-K-Means.ipynb)\n",
        "- [In Depth: Gaussian Mixture Models](05.12-Gaussian-Mixtures.ipynb)\n",
        "- [In-Depth: Kernel Density Estimation](05.13-Kernel-Density-Estimation.ipynb)\n",
        "- [Application: A Face Detection Pipeline](05.14-Image-Features.ipynb)\n",
        "- [Further Machine Learning Resources](05.15-Learning-More.ipynb)\n",
        "\n",
        "### [Appendix: Figure Code](06.00-Figure-Code.ipynb)"
      ]
    }
  ]
}